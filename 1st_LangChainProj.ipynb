{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiQxXIWIzGlZ13Xmu1MTAf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mkhan512/B63_Q2_Projects/blob/LangChain_Projects/1st_LangChainProj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Project 1: LangChain Hello World Project\n",
        "            In this Project, we will create a simple LangChain Colab Notebook that uses the Google Gemini Flash 2.0 model to answer user questions. This example below is provided to help you get started assumes you have access to the Gemini API and a basic Python environment. However, you are required to develop and submit your project using Google Colab."
      ],
      "metadata": {
        "id": "kjDcrBbn81yQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Set Up the Environment"
      ],
      "metadata": {
        "id": "u3zPBCPiwb2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y build-essential libtool autoconf python3-dev"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xDRxvNzoqP7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://libarchive.googlecode.com/files/libarchive-3.0.3.tar.gz\n",
        "!tar xzf libarchive-3.0.3.tar.gz"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Yftjo6RfpKW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd libarchive-3.0.3\n",
        "!./build/autogen.sh\n",
        "!./configure --prefix=/usr/local\n",
        "!make\n",
        "!make install"
      ],
      "metadata": {
        "id": "2eVbrU-Oo0EU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tT7FRAlVpt0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain.GOOGLE_genai"
      ],
      "metadata": {
        "collapsed": true,
        "id": "HXGEk7KQpAmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_google_genai"
      ],
      "metadata": {
        "collapsed": true,
        "id": "epXuin4Hwryh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gkmXEjAMxVh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install google-generative-ai"
      ],
      "metadata": {
        "id": "WQj3tTAzw66T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Configure the Gemini Flash Model"
      ],
      "metadata": {
        "id": "M9ZEUwwVwpry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "Gemini_API_key= userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "5bHX9-B4484j"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "3rS_gprX5J9z"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Create a Prompt Template"
      ],
      "metadata": {
        "id": "Q7b4k0hBxXQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_prompt = PromptTemplate(\n",
        "    input_variables = [\"Question\"],\n",
        "    template = \"Write a 5 key points {Question}\"\n",
        ")"
      ],
      "metadata": {
        "id": "CdpThm6tkqKt"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm= GoogleGenerativeAI(\n",
        "    api_key= Gemini_API_key,\n",
        "    model= \"gemini-1.5-flash\"\n",
        ")"
      ],
      "metadata": {
        "id": "uQOWjQs6t1A2"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 4: Build the LangChain Pipeline (first Cahin)"
      ],
      "metadata": {
        "id": "7ipHcEhMxjh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_chain = first_prompt | llm"
      ],
      "metadata": {
        "id": "3cvd-Zb4mj1q"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 5: Run the Hello World Example\n",
        "\n",
        "**Pass a sample question to the chain and print the response.**"
      ],
      "metadata": {
        "id": "mr7JvHQ8x2QX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = first_chain.invoke({\"Question\": \"what is Lang chain\"})\n",
        "# Split the response into lines (assuming bullet points are on separate lines)\n",
        "lines = response.split('\\n')\n",
        "# Print each line with a bullet point prefix and bold italic formatting\n",
        "for line in lines:\n",
        "    print(\"- \" + line + \"\") # Applying bold italic formatting using markdown syntax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7wb1bvK5g0T",
        "outputId": "9b999fd0-cf8a-4e84-c046-54f945e6816d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- LangChain's core functionality can be summarized in these 5 key points:\n",
            "- \n",
            "- 1. **Modular Application Development for LLMs:** LangChain provides a framework for building applications powered by Large Language Models (LLMs).  It offers modular components that can be easily combined and customized, simplifying the development process.\n",
            "- \n",
            "- 2. **Chain Composition:**  LangChain excels at chaining together different LLMs and other components (like prompts, memory, indexes) to create complex workflows. This allows for more sophisticated and nuanced applications beyond simple single-call interactions with an LLM.\n",
            "- \n",
            "- 3. **Memory Management:** LangChain handles the management of conversation history and context. This is crucial for applications requiring continuous interaction, ensuring the LLM maintains coherence and remembers previous exchanges.\n",
            "- \n",
            "- 4. **Data Connection:** LangChain facilitates the integration of LLMs with external data sources.  This allows applications to access and process information from databases, APIs, documents, and other sources, significantly expanding the LLM's capabilities.\n",
            "- \n",
            "- 5. **Agent Capabilities:** LangChain supports the creation of agents that can autonomously interact with their environment (including LLMs and external tools) to achieve specific goals. These agents can choose which tools to use and how to use them based on the task at hand.\n",
            "- \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9V7AX7Tg8rO1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7sih0Hv28yJN"
      }
    }
  ]
}